{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dAoqqWmnnUn3alLK0qPvgwNgecFC-ZI5",
      "authorship_tag": "ABX9TyNzJjZ/e3QyJXxJ62/iCF1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manhhad32/nlp/blob/master/SearchUsePhoBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygKbl7bGez_t",
        "outputId": "85368223-3338-4019-c203-66bc42f5da7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Tải PhoBERT-large và tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-large\")\n",
        "model = AutoModel.from_pretrained(\"vinai/phobert-large\")\n",
        "\n",
        "# Sử dụng GPU nếu có\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16GfkFqSf-Tw",
        "outputId": "fbcfaf6f-f609-4c6c-9e1d-5bea59ee2c1a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(64001, 1024, padding_idx=1)\n",
              "    (position_embeddings): Embedding(258, 1024, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-23): 24 x RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(text):\n",
        "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=100).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "    # Tính trung bình các vector đầu ra cho tất cả các tokens\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "# Hàm để mã hóa văn bản dài bằng cách chia thành các chunk nhỏ\n",
        "def encode_long_text(text, chunk_size=512):\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
        "\n",
        "    chunk_vectors = []\n",
        "    for chunk in chunks:\n",
        "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
        "        chunk_vector = encode_text(chunk_text)\n",
        "        chunk_vectors.append(chunk_vector)\n",
        "\n",
        "    # Tính trung bình của tất cả các vector chunk để biểu diễn toàn bộ văn bản\n",
        "    return torch.tensor(chunk_vectors).mean(dim=0).numpy()\n",
        "\n",
        "# Hàm tính độ tương đồng cosine\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
      ],
      "metadata": {
        "id": "NXZbeDIqgA9_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Đường dẫn đến thư mục chứa các file document\n",
        "doc_folder = '/content/drive/MyDrive/datanlp/'\n",
        "\n",
        "# Đọc và mã hóa tất cả các tài liệu trong thư mục\n",
        "document_vectors = []\n",
        "documents = []\n",
        "fi_names = []\n",
        "def load_documents_from_directory(directory):\n",
        "    print('Loading documents from directory')\n",
        "    docs = []\n",
        "    for file_name in os.listdir(directory):\n",
        "        fi_names.append(file_name)\n",
        "        print('Đọc dữ liệu từ file: [{}]...'.format(file_name))\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        lines = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as fi:\n",
        "            # reade file acording lines.\n",
        "            for line in fi:\n",
        "                # change to lower character and remove space at head/tail of line.\n",
        "                line = line.lower().strip()\n",
        "                # add to list\n",
        "                lines.append(line)\n",
        "        # create doc as text inline.\n",
        "        doc = \" \".join(lines)\n",
        "        # clean, remove some spec word in head/tail.\n",
        "        re.sub('\\W+',' ', doc)\n",
        "        docs.append(doc)\n",
        "    print('End Loading documents from directory')\n",
        "    return docs\n",
        "\n",
        "documents = load_documents_from_directory(doc_folder)\n",
        "\n",
        "for doc in documents:\n",
        "  document_vectors.append(encode_long_text(doc))\n",
        "#for filepath in glob.glob(doc_folder):\n",
        "#    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "#        text = file.read()\n",
        "#        documents.append(text)\n",
        "#        document_vectors.append(encode_long_text(text))\n",
        "\n",
        "document_vectors = torch.tensor(document_vectors)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eEbFEy-gEUl",
        "outputId": "0edfc8d1-c564-4d60-80b7-374699b08661"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents from vnexpress directory\n",
            "Đọc dữ liệu từ file: [ba-ly-do-nen-di-phu-quoc-ngay-va-luon-4528757.txt]...\n",
            "Đọc dữ liệu từ file: [banh-lot-xao-trung-mon-an-campuchia-hut-khach-viet-4527571.txt]...\n",
            "Đọc dữ liệu từ file: [ba-mon-ca-cho-bua-toi-se-lanh-o-dien-bien-4527828.txt]...\n",
            "Đọc dữ liệu từ file: [6-khach-san-viet-vao-top-sang-trong-nhat-dong-nam-a-4529357.txt]...\n",
            "Đọc dữ liệu từ file: [10-khu-luu-tru-viet-thang-giai-sang-trong-cua-chau-a-4527584.txt]...\n",
            "Đọc dữ liệu từ file: [ben-trong-du-thuyen-dau-gia-hon-35-ty-dong-cua-flc-4527411.txt]...\n",
            "Đọc dữ liệu từ file: [cac-diem-den-bi-an-o-viet-nam-cho-ngay-halloween-4527952.txt]...\n",
            "Đọc dữ liệu từ file: [chang-ha-noi-tp-hcm-co-quay-check-in-va-nhan-hanh-ly-rieng-4529038.txt]...\n",
            "Đọc dữ liệu từ file: [cach-sun-group-nang-tam-du-lich-hon-thom-4528015.txt]...\n",
            "Đọc dữ liệu từ file: [cho-ngoi-tot-va-te-nhat-tren-may-bay-4528373.txt]...\n",
            "Đọc dữ liệu từ file: [bon-yeu-cau-khien-tiep-vien-kho-chiu-nhat-4528270.txt]...\n",
            "Đọc dữ liệu từ file: [co-gai-di-khap-noi-chi-voi-mot-chiec-tui-deo-vai-4529233.txt]...\n",
            "Đọc dữ liệu từ file: [co-gai-goc-viet-di-99-nuoc-tren-the-gioi-4529800.txt]...\n",
            "Đọc dữ liệu từ file: [co-gai-mien-nam-uoc-dap-xe-chinh-phuc-tu-dai-dinh-deo-4528346.txt]...\n",
            "Đọc dữ liệu từ file: [co-nen-du-lich-cung-nhau-truoc-khi-cuoi-4528982.txt]...\n",
            "Đọc dữ liệu từ file: [cua-ghe-song-ngam-tuong-la-dac-san-o-nuoc-nao-4529451.txt]...\n",
            "Đọc dữ liệu từ file: [da-lat-thu-nho-cua-hai-phong-4529366.txt]...\n",
            "Đọc dữ liệu từ file: [dao-nam-du-vao-mua-dep-cuoi-nam-4530232.txt]...\n",
            "Đọc dữ liệu từ file: [cung-duong-mua-thu-nuoc-my-4528354.txt]...\n",
            "Đọc dữ liệu từ file: [du-khach-tp-hcm-an-tuong-voi-ve-dep-cua-nam-dinh-4526769.txt]...\n",
            "Đọc dữ liệu từ file: [hang-tram-du-khach-tu-kazakhstan-bay-thang-den-khanh-hoa-4528245.txt]...\n",
            "Đọc dữ liệu từ file: [den-dien-bien-an-xoi-san-com-lam-nep-nuong-4530187.txt]...\n",
            "Đọc dữ liệu từ file: [hoi-an-binh-yen-sau-nhung-con-mua-bao-4527567.txt]...\n",
            "Đọc dữ liệu từ file: [hong-kong-bo-han-che-gio-choi-dem-4529518.txt]...\n",
            "Đọc dữ liệu từ file: [huong-di-cho-quang-nam-phat-trien-du-lich-trai-mua-4516307.txt]...\n",
            "Đọc dữ liệu từ file: [itaewon-pho-tay-khong-ngu-o-han-quoc-4529823.txt]...\n",
            "Đọc dữ liệu từ file: [khach-viet-khong-the-bay-toi-gangwon-dien-mien-visa-4527354.txt]...\n",
            "Đọc dữ liệu từ file: [le-hoi-kate-cua-nguoi-cham-4527801.txt]...\n",
            "Đọc dữ liệu từ file: [mat-trai-cua-trao-luu-cam-trai-4529913.txt]...\n",
            "Đọc dữ liệu từ file: [mon-mi-lam-tu-dau-nanh-cua-nguoi-hoa-4530692.txt]...\n",
            "Đọc dữ liệu từ file: [mot-ngay-di-het-cac-diem-den-moi-o-mang-den-4528575.txt]...\n",
            "Đọc dữ liệu từ file: [mua-thu-binh-yen-o-vung-song-nuoc-bi-4529129.txt]...\n",
            "Đọc dữ liệu từ file: [mua-lua-chin-o-pu-luong-4528845.txt]...\n",
            "Đọc dữ liệu từ file: [nam-ngay-chon-chan-tren-nui-tuyet-nepal-4527723.txt]...\n",
            "Đọc dữ liệu từ file: [ngoi-lang-nguoi-dan-song-tho-vi-khong-bi-cang-thang-4529642.txt]...\n",
            "Đọc dữ liệu từ file: [nhung-diem-den-ngam-hoa-thang-11-4530527.txt]...\n",
            "Đọc dữ liệu từ file: [noi-nguoi-dan-an-mai-khong-beo-4527089.txt]...\n",
            "Đọc dữ liệu từ file: [phi-cong-tiet-lo-o-cua-so-tren-may-bay-co-the-mo-4529950.txt]...\n",
            "Đọc dữ liệu từ file: [no-luc-di-khap-dong-nam-a-cua-co-gai-mac-nhieu-benh-4526774.txt]...\n",
            "Đọc dữ liệu từ file: [pho-di-bo-tp-hcm-ha-noi-nhon-nhip-dip-halloween-4530066.txt]...\n",
            "Đọc dữ liệu từ file: [quan-ca-phe-o-sa-pa-ban-duy-nhat-mot-loai-do-uong-4527657.txt]...\n",
            "Đọc dữ liệu từ file: [quan-mi-hat-nuoc-leo-len-tran-nha-de-hut-khach-4529688.txt]...\n",
            "Đọc dữ liệu từ file: [san-ve-tet-nguyen-dan-uu-dai-den-90-cua-vietjet-4530716.txt]...\n",
            "End documents from vnexpress directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mã hóa câu truy vấn\n",
        "query = \"Hà Nội, TP HCM, Phú Yên, Quảng Ninh và Phú Quốc là những nơi có các khu nghỉ dưỡng, khách sạn được vinh danh tại lễ trao giải World Luxury Hotel Awards.\"\n",
        "query_vector = encode_long_text(query)\n",
        "\n",
        "# Tính toán Cosine Similarity\n",
        "similarities = []\n",
        "for doc_vector in document_vectors:\n",
        "    similarities.append(cosine_similarity(query_vector, doc_vector))\n",
        "\n",
        "\n",
        "# Sắp xếp các tài liệu theo độ tương đồng giảm dần\n",
        "doc_fi_names = zip(documents, fi_names)\n",
        "sorted_docs = sorted(zip(doc_fi_names, similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# In ra các tài liệu có độ tương đồng cao nhất\n",
        "top_k = 5  # Số lượng tài liệu muốn hiển thị\n",
        "result_doc = []\n",
        "for doc, similarity in sorted_docs[:top_k]:\n",
        "    result_doc.append(f\"Tài liệu: {tuple(doc)[1]}... (độ tương đồng: {similarity:.4f})\")\n",
        "    print(f\"Tài liệu: {tuple(doc)[1]}... (độ tương đồng: {similarity:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbgBE9ZIgHCF",
        "outputId": "a8f948a7-d945-4d16-dfc4-a8673f72ae3b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tài liệu: 6-khach-san-viet-vao-top-sang-trong-nhat-dong-nam-a-4529357.txt... (độ tương đồng: 0.9905)\n",
            "Tài liệu: 10-khu-luu-tru-viet-thang-giai-sang-trong-cua-chau-a-4527584.txt... (độ tương đồng: 0.9886)\n",
            "Tài liệu: cach-sun-group-nang-tam-du-lich-hon-thom-4528015.txt... (độ tương đồng: 0.9868)\n",
            "Tài liệu: mat-trai-cua-trao-luu-cam-trai-4529913.txt... (độ tương đồng: 0.9866)\n",
            "Tài liệu: huong-di-cho-quang-nam-phat-trien-du-lich-trai-mua-4516307.txt... (độ tương đồng: 0.9862)\n"
          ]
        }
      ]
    }
  ]
}