{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Mô Hình PhoBERT\n",
    "\n",
    "PhoBERT là một phiên bản BERT dành riêng cho tiếng Việt, được phát triển bởi VinAI Research. Nó dựa trên kiến trúc Transformer và đã chứng minh hiệu suất vượt trội trên nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP) tiếng Việt.\n",
    "\n",
    "1. Cơ Sở Lý Thuyết\n",
    "\n",
    "Kiến Trúc Transformer\n",
    "PhoBERT được xây dựng trên kiến trúc Transformer, bao gồm hai phần chính:\n",
    "\n",
    "Encoder: Đầu vào là các token của câu, qua các lớp self-attention và feed-forward để mã hóa thông tin.\n",
    "Decoder: Không được sử dụng trong PhoBERT vì nó chỉ là mô hình mã hóa.\n",
    "BERT (Bidirectional Encoder Representations from Transformers)\n",
    "BERT là mô hình được huấn luyện trước trên hai nhiệm vụ chính:\n",
    "\n",
    "Masked Language Modeling (MLM): Một số từ trong câu được che đi, và mô hình phải đoán từ đó dựa trên ngữ cảnh xung quanh.\n",
    "Next Sentence Prediction (NSP): Xác định xem một câu có phải là câu tiếp theo của câu trước đó hay không.\n",
    "PhoBERT\n",
    "PhoBERT sử dụng cùng kiến trúc và phương pháp huấn luyện như BERT, nhưng được huấn luyện trên dữ liệu tiếng Việt. Điều này bao gồm các bước:\n",
    "\n",
    "Tokenization: Sử dụng Byte-Pair Encoding (BPE) để tạo các token từ văn bản tiếng Việt.\n",
    "Huấn Luyện MLM: Che giấu các từ ngẫu nhiên và huấn luyện mô hình để dự đoán các từ này.\n",
    "Huấn Luyện NSP: Huấn luyện mô hình để dự đoán mối quan hệ giữa hai câu liên tiếp.\n",
    "PhoBERT có hai phiên bản:\n",
    "\n",
    "PhoBERT-base: 12 layers, 768 hidden units, 12 attention heads, và khoảng 135 triệu tham số.\n",
    "PhoBERT-large: 24 layers, 1024 hidden units, 16 attention heads, và khoảng 370 triệu tham số.\n",
    "\n",
    "\n",
    "2. ví dụ code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: torch in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (1.12.1)\n",
      "Requirement already satisfied: filelock in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: typing_extensions in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from torch) (4.7.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.3.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nguyenmanhha/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.1-cp310-cp310-macosx_11_0_arm64.whl (385 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.2/385.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.3.4-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, requests, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.11.0\n",
      "    Uninstalling fsspec-2022.11.0:\n",
      "      Successfully uninstalled fsspec-2022.11.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.1\n",
      "    Uninstalling huggingface-hub-0.10.1:\n",
      "      Successfully uninstalled huggingface-hub-0.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
      "spyder 5.4.1 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.1 requires pyqtwebengine<5.16, which is not installed.\n",
      "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires requests==2.28.1, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.3.4 aiohttp-3.10.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 huggingface-hub-0.24.5 multidict-6.0.5 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 requests-2.32.3 tqdm-4.66.5 xxhash-3.4.1 yarl-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "#data = pd.read_csv('data.csv')\n",
    "\n",
    "# Giả sử dữ liệu có hai cột 'text' và 'label'\n",
    "data = {\n",
    "    'text': [\"Tôi yêu học ngôn ngữ tự nhiên\", \"Ngôn ngữ tự nhiên rất thú vị\", \"Học ngôn ngữ tự nhiên giúp tôi hiểu văn bản tốt hơn\"],\n",
    "    'label': [1, 0, 1]\n",
    "}\n",
    "\n",
    "# Chuyển đổi dữ liệu thành định dạng Dataset của Hugging Face\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiền Xử Lý Dữ Liệu\n",
    "Sử dụng tokenizer của PhoBERT để mã hóa văn bản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Sử dụng tokenizer của PhoBERT\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvinai/phobert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Sử dụng tokenizer của PhoBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# Hàm mã hóa\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Áp dụng hàm mã hóa cho tập dữ liệu\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Đặt định dạng cho tập dữ liệu\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huấn Luyện PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Tạo mô hình PhoBERT cho nhiệm vụ phân loại\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2)\n",
    "\n",
    "# Định nghĩa các tham số huấn luyện\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Tạo Trainer để huấn luyện mô hình\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "# Bắt đầu huấn luyện\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đánh Giá Mô Hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá mô hình trên tập kiểm tra\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
