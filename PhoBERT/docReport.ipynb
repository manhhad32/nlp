{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Mô Hình PhoBERT\n",
    "\n",
    "PhoBERT là một phiên bản BERT dành riêng cho tiếng Việt, được phát triển bởi VinAI Research. Nó dựa trên kiến trúc Transformer và đã chứng minh hiệu suất vượt trội trên nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP) tiếng Việt.\n",
    "\n",
    "1. Cơ Sở Lý Thuyết\n",
    "\n",
    "Kiến Trúc Transformer\n",
    "PhoBERT được xây dựng trên kiến trúc Transformer, bao gồm hai phần chính:\n",
    "\n",
    "Encoder: Đầu vào là các token của câu, qua các lớp self-attention và feed-forward để mã hóa thông tin.\n",
    "Decoder: Không được sử dụng trong PhoBERT vì nó chỉ là mô hình mã hóa.\n",
    "BERT (Bidirectional Encoder Representations from Transformers)\n",
    "BERT là mô hình được huấn luyện trước trên hai nhiệm vụ chính:\n",
    "\n",
    "Masked Language Modeling (MLM): Một số từ trong câu được che đi, và mô hình phải đoán từ đó dựa trên ngữ cảnh xung quanh.\n",
    "Next Sentence Prediction (NSP): Xác định xem một câu có phải là câu tiếp theo của câu trước đó hay không.\n",
    "PhoBERT\n",
    "PhoBERT sử dụng cùng kiến trúc và phương pháp huấn luyện như BERT, nhưng được huấn luyện trên dữ liệu tiếng Việt. Điều này bao gồm các bước:\n",
    "\n",
    "Tokenization: Sử dụng Byte-Pair Encoding (BPE) để tạo các token từ văn bản tiếng Việt.\n",
    "Huấn Luyện MLM: Che giấu các từ ngẫu nhiên và huấn luyện mô hình để dự đoán các từ này.\n",
    "Huấn Luyện NSP: Huấn luyện mô hình để dự đoán mối quan hệ giữa hai câu liên tiếp.\n",
    "PhoBERT có hai phiên bản:\n",
    "\n",
    "PhoBERT-base: 12 layers, 768 hidden units, 12 attention heads, và khoảng 135 triệu tham số.\n",
    "PhoBERT-large: 24 layers, 1024 hidden units, 16 attention heads, và khoảng 370 triệu tham số.\n",
    "\n",
    "\n",
    "2. ví dụ code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Chuyển đổi dữ liệu thành định dạng Dataset của Hugging Face\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiền Xử Lý Dữ Liệu\n",
    "Sử dụng tokenizer của PhoBERT để mã hóa văn bản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Sử dụng tokenizer của PhoBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# Hàm mã hóa\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Áp dụng hàm mã hóa cho tập dữ liệu\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Đặt định dạng cho tập dữ liệu\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huấn Luyện PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Tạo mô hình PhoBERT cho nhiệm vụ phân loại\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2)\n",
    "\n",
    "# Định nghĩa các tham số huấn luyện\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Tạo Trainer để huấn luyện mô hình\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "# Bắt đầu huấn luyện\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đánh Giá Mô Hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá mô hình trên tập kiểm tra\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
