{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Khởi tạo tokenizer và mô hình PhoBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# Hàm mã hóa văn bản dài\n",
    "def encode_long_text(text, tokenizer, model, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    chunk_vectors = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(\" \".join(chunk), return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        chunk_vector = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        chunk_vectors.append(chunk_vector)\n",
    "    return chunk_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ danh sách các tài liệu dài\n",
    "documents = [\"Tài liệu dài số 1 ...\", \"Tài liệu dài số 2 ...\", ...]\n",
    "\n",
    "# Mã hóa và lưu trữ các vector biểu diễn\n",
    "document_vectors = []\n",
    "for doc in documents:\n",
    "    vectors = encode_long_text(doc, tokenizer, model)\n",
    "    document_vectors.append(vectors)\n",
    "\n",
    "# Lưu trữ vector vào file để sử dụng lại sau này\n",
    "with open('document_vectors.json', 'w') as f:\n",
    "    json.dump(document_vectors, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bước 3: Mã Hóa Câu Truy Vấn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Câu truy vấn\n",
    "query = \"Tôi muốn tìm hiểu về ngôn ngữ tự nhiên\"\n",
    "\n",
    "# Mã hóa câu truy vấn\n",
    "query_tokens = tokenizer.tokenize(query)\n",
    "query_inputs = tokenizer(\" \".join(query_tokens), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "with torch.no_grad():\n",
    "    query_outputs = model(**query_inputs)\n",
    "query_vector = query_outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bước 4: Tính Toán Độ Tương Đồng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tính độ tương đồng cosine\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Tính toán độ tương đồng giữa câu truy vấn và các đoạn của mỗi tài liệu\n",
    "similarities = []\n",
    "for doc_vectors in document_vectors:\n",
    "    doc_similarities = [cosine_similarity(query_vector, vec) for vec in doc_vectors]\n",
    "    max_similarity = max(doc_similarities)\n",
    "    similarities.append(max_similarity)\n",
    "\n",
    "# Sắp xếp các tài liệu theo độ tương đồng giảm dần\n",
    "sorted_docs = sorted(zip(documents, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# In ra các tài liệu có độ tương đồng cao nhất\n",
    "for doc, similarity in sorted_docs[:10]:  # Top 10 tài liệu tương đồng nhất\n",
    "    print(f\"Tài liệu: {doc[:100]}... (độ tương đồng: {similarity:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
